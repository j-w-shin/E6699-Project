\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
% \usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{neurips_2019}

% to avoid loading the natbib package, add option nonatbib:
\usepackage[nonatbib]{neurips_2019}
\usepackage{comment}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{bold-extra}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{array}

\title{Overparameterization, Batch Norm, and the Attack of the Confused Gradients}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Zongkai Tian \\
  Department of Computer Science\\
  Columbia University\\
  116 th St & Broadway, New York, NY 10027 \\
  \texttt{zt2218@columbia.edu} \\
  % examples of more authors
  \And
   James Shin\\
   Department of Computer Science\\
   Columbia University\\
   116 th St & Broadway, New York, NY 10027 \\
  \texttt{js4785@columbia.edu} \\
  \AND
  Ruisi Wang\\
  Department of Computer Science\\
  Columbia University\\
  116 th St & Broadway, New York, NY 10027 \\
  \texttt{rw2720@columbia.edu} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle
\begin{abstract}
\textit{Overparameterization}
\end{abstract}

\section{Introduction}
Neural networks have seen remarkable progress in recent years in terms of approximating complex functions using various optimization methods such as stochastic gradient descent. In particular, neural networks in practice have shown how \textit{overparameterization} (an increase in parameters of a model over the number of training data) and \textit{batch normalization}, are able to make complex neural networks easier to train. However, prior to recent research, not much was understood in terms of concretely defining why these methods work so well to speed up training. \textit{Why, precisely, do overparameterization and batch normalization allow for quicker convergence to global and local minima?} In this paper we address this question by investigating a recently-developed concept known as \textit{gradient confusion}, a measure of differences between stochastic gradients of each data sample in a training set, that causes convergence to slow down. We also will discuss overparameterization and batch normalization in-depth, which can be shown (both empirically and theoretically) to lower gradient confusion and thus speed up training.

\section{Gradient Confusion}
We first define gradient confusion in the context of stochastic gradient descent (SGD). Given $N$ training points $X = x_1,...,x_N$, we proceed with gradient descent, back-propagating for each training point. Using these training points, we can specify $N$ loss functions, $\{f\}_{i \in [N]}$. Then we use SGD to solve the empirical risk minimization problem; that is, we hope to find optimal solution
$$w^* = \text{arg min}_{w \in \mathbb{R}^d} \frac{1}{N} \sum_{i=1}^N f_i (w).$$
As SGD stochastically chooses a training example and performs gradient updates, the selected gradient $\nabla f_i$ for training example $x_i$ may very well be negatively correlated with another gradient term $\nabla f_j$ for example $x_j$. This negative correlation is defined to have \textit{gradient confusion}. \\

More formally, a set of objective functions $\{f_i\}_{i \in [N]}$ is said to have a $\textit{gradient confusion bound}$ $\eta \geq 0$ if, for fixed $w \in \mathbb{R}^d$,

$$ \nabla f_i(w) \bullet \nabla f_j(w) \geq -\eta, \;\; \forall i \not = j \in [N]$$.

That is, the inner product of the gradient vectors of every pair of unique loss functions is bounded by $-\eta$.

\subsection{Intuition}
Intuitively, low gradient confusion would imply that an epoch of SGD over an entire dataset leads to faster training, because most of the gradient updates of the weights would be aligned with each other. In practice, in high dimensions, \textit{randomly-chosen vectors are nearly orthogonal with high probability} \cite{Milman}; so, we might reasonably expect the \textit{average} randomly-chosen set of training examples to have approximately orthogonal gradients, as long as the number of parameters is large, and the number of training data is small, in order to minimize the probability of encountering gradient confusion. 

This may hold on expectation, but it is difficult to ensure low gradient confusion without more rigorous reasoning. As such, we present general theories and guarantees of overparameterizing and batch-normalizing a network, as well as empirical experiments, in order to show that low gradient confusion indeed holds. This analysis would then concretely explain why training is sped up when these methods are employed, and also hopefully shed light into how methods can be designed in order to speed up training, which is an incredibly expensive task.

\subsection{The effect of network layer width on gradient confusion}
We can also introduce a quantity related to gradient confusion, known as \textit{gradient diversity}, proposed by Dong, et al. \cite{dong}, which is a similar measure of the diversity of gradients of loss functions; in this paper, they used this quantity to put bounds on batch sizes for data sets. It is defined as follows:

$$\triangle_s(w) = \frac{\sum_{i=1}^n || \nabla f_i(w) ||_2^2}{\sum_{i=1}^n || \nabla f_i(w)||_2^2 + \sum_{i \not = j} \nabla f_i(w) \bullet \nabla f_j(w) }$$

As we can see, gradient confusion is present in the denominator. If every gradient was the same, $\triangle_s(w) = 1$; on the contrary, $\triangle_s(w)$ shrinks as gradient confusion grows. \\

Now, consider a simple two-layer linear network $g(\textbf{x}) = \textbf{W}_1 \textbf{W}_0 \textbf{x}$, with $\textbf{x} \in \mathbb{R}^d$, $g(\textbf{x}) \in \mathbb{R}$, $\textbf{W}_1 \in \mathbb{R}^{l \times d}$, and $\textbf{W}_0 \in \mathbb{R}^{1 \times l}$. Also, suppose element $w_{ij} \stackrel{}{\sim} \text{Normal}(0, 1/l)$, $\forall w_{ij} \in \textbf{W}_0, \textbf{W}_1$, and also $x_i \in \textbf{x} \stackrel{}{\sim} \text{Normal}(0, 1/d)$. Chen, et al. \cite{chen} provides a bound, which can be further simplified as the following:
$$ \triangle_s(w) \geq \frac{1}{n} \frac{\Theta (nld)}{\Theta (ln + dn + ld)} = \frac{\Theta(ld)}{\Theta(ln + dn + ld)} \approx \frac{\Theta(ld)}{\Theta(l + d)}$$ 
for sufficiently large $n$. Assuming least-square losses we then have $\mathbb{E}_{\textbf{W}, \textbf{x}}[\nabla f_i \bullet \nabla f_j] = \Theta(\frac{1}{l})$, $\forall x_i, x_j$. We also have a bound on variance, which is $\text{var}_{\textbf{W}, \textbf{x}}(\nabla f_i \bullet \nabla f_j) \leq \mathbb{E}_{\mathbf{W}, \mathbf{x}} [||\nabla f_i||^2 ||\nabla f_j||]^2 = O(\frac{1}{l^2})$. This implies that width $l$ and gradient confusion have an inverse relationship.\\

As such, overparameterizing a network by increasing width certainly reduces gradient confusion, thus leading to faster training times. In order to precisely understand why this is the case from an optimization point of view, we dive deeper into work by S. Du, et al. which discusses how gradient descent can provably optimize overparametrized networks.

\section{Over-parameterization}
We survey previous work on optimization and generalization aspects of two-layer neural networks, which mainly focus on how they converge to global minimum and generalize in the overpameterized setting. \textsl{Gradient Descent Provably Optimizes over-parameterized neural networks} by Du et al. and \textsl{Fine-Grained Analysis of Optimization and Generalization for Overparameterized Tow-layer Neural Networks} by Arora et al. mainly discuss gradient descent convergence, while \textsl{A Priori Estimates For Two-layers Neural Networks} by Arora et al. and \textsl{A Comparative Analysis of the Optimization and Generalization Property of Two-layer Neural Network and Random Feature Models Under Gradient Descent Dynamics} by Weinan discuss about generalization bounds. 

\subsection{Gradient Descent Provably Optimizes over-parameterized neural networks}
One empirical observation of network training is that, even though optimization objective function may be non-convex and non-smooth, randomly initialized first-order methods such as stochastic gradient descent can still find global minimums. Surprisingly, this property is not correlated with labels. A widely-believed explanation on why several neural networks can fit all training labels is that the networks are over-parameterized. \\
In this paper, we rigorously prove that as long as no two inputs are parallel and m is large enough, with randomly initialized $a$ and $\textbf{\textsc{W(0)}}$, gradient descent achieves zero training loss at a linear convergence rate, i.e., it finds a solution $\textbf{\textsc{W(K)}}$ with $L(\textbf{\textsc{W(K)}}) \leq \epsilon$ in $K = O(\log(1/\epsilon))$ iterations.

Convergence Rate of Gradient Flow,
Continuous time analysis
Theorem 3.2 Suppose Assumption 3.1 (if for any $i \neq j,x_i \nparallel x_j, then \lambda_0 >0$) holds and for all $i \in [n],\|x_i\|_{2} = 1$ and $|y_i|\leq C$ for some constant C, then if we set the number of hidden nodes $m=\Omega (\frac{n^6}{\lambda^4_0 \sigma ^3})$ and we i,i,d initialize $w_r\sim n(0,I),a_r \sim unif[{-1,1}] for r\in [m]$, then with probability at least $1-\sigma$ over the initialization, we have 
\begin{center}
    $\|u(t)-y\|^2_2 \leq exp(-\lambda_0 t)\|u(0)-y\|^2_2$
\end{center}

This theorem establishes that if m is large enough, the training error converges to 0 at a linear rate. Here we assume $\|x_i\|_2 = 1$ only for simplicity and it is not hard to relax this condition. The bounded label condition also holds for most real world data set. The number of hidden nodes m required is $\omega(\frac{n^6}{\lambda^4_0 \sigma ^3})$,which depends on the number of samples $n$, $\lambda_0$, and the failure probability $\sigma$. Over-parameterization,i.e.,the fact $m = poly(n,1/{\lambda_0},1/{\sigma}$,plays a crucial role in gradient descent to find the global minimum. 

discrete time analysis
Suppose Assumption 3.1 (if for any $i \neq j,x_i \nparallel x_j, then \lambda_0 >0$) holds and for all $i \in [n],\|x_i\|_{2} = 1$ and $|y_i|\leq C$ for some constant C, then if we set the number of hidden nodes $m=\Omega (\frac{n^6}{\lambda^4_0 \sigma ^3})$ and we i,i,d initialize $w_r\sim n(0,I),a_r \sim unif[{-1,1}] for r\in [m]$, then with probability at least $1-\sigma$ over the initialization, we have 
\begin{center}
    $\|u(t)-y\|^2_2 \leq (1-\frac{\eta \lambda_0}{2})^k\|u(0)-y\|^2_2$
\end{center}
It shows even though the objective function is non-smooth and non-convex, gradient descent with a constant step size still enjoys a linear convergence rate.

By using synthetic data, experiments have been done from three aspects: 
\begin{itemize}
\item how the amount of over-parameterization affects the convergence rates.
\item the relation between the amount of over-parameterization and the number of pattern changes.
\item the relation between the amount of over-parameterization and the maximum of the distances between weight vectors and their initialization. 
\end{itemize}
And the result shows as m becomes larger, convergence rate is better;the percentiles of pattern changes and the maximum distance from the initialization become smaller.

However, there are more directions for the further research. 
\begin{itemize}
\item the generalization to deep neural networks with more hidden layers.
\item the number of hidden nodes m required can be reduced. 
\item instead of using empirical loss, another potential function maybe able to be used to prove the convergence rates of accelerated methods. 
\end{itemize}

\subsection{Fine-Grained Analysis of Optimization and Generalization for Overparameterized Tow-layer Neural Networks}
Previous works shows that sufficiently powerful nets(with vastly more parameters than number of training samples) can attain zero training error, regardless of whether the data is properly labeled or randomly labeled.The gradient descent can allow an overparameterized multi-layer net to attain arbitrarily low training error on fairly generic datasets.

This paper accurately estimate the magnitude of training loss in each iteration. The key finding is that the number of iterations needed to achieve a target accuracy depends on the projections of data labels on the eigenvectors of a certain Gram matrix. Also, it give a generalization bound for the solution found by GD, based on accurate estimates of how much the network parameters can move during optimization. The generalization bound depends on a data-dependent complexity measure and notably, is completely independent of the number of hidden units in the network.

As to Generalization bounds, the well known VC-dimension of neural networks is at least linear in the number of parameters and therefore classical VC theory cannot explain the generalization ability of modern neural networks with more parameters than training samples. PAC-Bayes approach to compute non-vacuous generalization bounds for MNIST and ImageNet, respectively. All these bounds depend on certain properties of the trained neural networks. In the paper, one of main results shows :

For any 1-Lipschitz loss function, the generalization error of the two-layer ReLU network found by GD is at most 
\begin{center}
  $\sqrt{\frac{2y^T(H^\infty)^-1y}{n}}$
\end{center}
In that case, fix an error parameter $\epsilon > 0$ and failure probability $\sigma \in (0,1)$ Suppose we have data $S={(x_i,y_i)}_{i=1}^n$ are i.i.d. samples from $a (\lambda_0,\frac{\sigma}{3},n)$ - non-degenerate distribution D, and $k = O(\frac{\epsilon\lambda_0\sigma}{\sqrt{n}}$,$m \geq K^{-2}poly(n,\lambda_0^{-1},\sigma^{-1},\epsilon^{-1})$.Consider any loss function $l:R*R ->[0,1]$ that is 1-Lipschitz in the first argument such that $l(y,y)=0 $Then with probability at least $1-\sigma$ over the random initialization and training examples, the two-layer neural network $f_{w(k),a}$ trained by GD for $k\geq\omega(\frac{1}{\mu^{\lambda_0}}\log{\frac{1}{\sigma_\epsilon}})$ iterations has population loss $L_D(f_{W(k),a})= E_{(x,y)~D}[l(f_{W(k)},a(X),y)]$ bounded as
\begin{center}
    $L_D(f{W(k),a})\leq \sqrt{\frac{2y^T(H^\infty)^-1y}{n}} +3\sqrt{\frac{\log(\frac{6}{\sigma}}){2n}}+\epsilon$
\end{center}
\subsection{A Priori Estimates For Two-layers Neural Networks}
We provide a perspective for understanding why two-layer neural networks perform better than kernel methods. We focus on two -layer networks, and we consider models with explicit regularization. We establish estimates for the population risk which are symptotically sharp with constants depending only on the properties of the target function. 
\begin{itemize}
\item We establish a priori estimates of the population risk for learning two -layer neural networks with an explicit regulation. 
\item We make a detailed comparison between the neural network and kernel methods using these a priori estimates. 
\end{itemize}

For a two-layer neural network $f(x;\theta)$ of width m, we define the regularized risk as 
\begin{center}
  ${\MakeUppercase{J}_\lambda}(\theta) : = \hat{\MakeUppercase{L}_n}(\theta)+ \lambda(\|\theta\|_P+1)$
\end{center}
The +1 term at the right hand side is included only to simplify the proof. Our result also holds if we do not indclude this term in  the regularized risk. The corresponding regularized estimator is defined as 
\begin{center}
  $\hat{\theta_{n,\lambda}} = argmin{{\MakeUppercase{J}_\lambda}(\theta)}$
\end{center}

For any $f \in barron space(\omega)$,there exists a two-layer neural network $f(x;\hat{\theta}$ of width m with $\|\Hat{\theta}_P\leq 2\gamma_2(f)$, such that
\begin{center}
  $E_x[(f(x)-f(x;\Hat{\theta}))^2]\leq\frac{3\gamma_2^2(f)}{m}$
\end{center}
The basic intuition is that the integral representation of f allows us to approximate f by the Monte-Carlo method:$f(x) \approx\frac{1}{m}\sum_{k=1}^{m} a(w_k){\sigma(<w_k,x>)}$ where${W_k}_{k=1}^m$ are sampled from the distribution $\pi$

The main result shows 
\begin{itemize}
\item For noiseless case, Assume that the target function $f^* /in B_2(\omega)$ and $\lambda \geq \lambda_n$. Then for any $\sigma > 0 $ and, the probability at least $1-\sigma$ over the choice of the training set $S$, we have
    \begin{center}
      $E_x[(f(x;\Hat{\theta})-f^*(x))^2]\leq\frac{\gamma_2^2(f^*)}{m} +\lambda \Hat{\gamma_2}(f^*)+\frac{1}{\sqrt{n}}(\Hat{\gamma_2}(f^*)+\sqrt{\ln{\frac{n}{\sigma}}})$
    \end{center}
And the population risk for the kernel methods should be much larger than the population risk for the neural network.
\item For noisy case, Assume that the target function $f^* /in B_2(\omega)$ and $\lambda \geq \lambda_n$. Then for any $\sigma > 0 $ and, the probability at least $1-\sigma$ over the choice of the training set $S$, we have
      $E_x[(f(x;\Hat{\theta})-f^*(x))^2]\leq\frac{\gamma_2^2(f^*)}{m} +\lambda B_n \Hat{\gamma_2}(f^*)+\frac{B^2_n}{\sqrt{n}}(\Hat{\gamma_2}(f^*)+\sqrt{\ln{\frac{n}{\sigma}}})+\frac{B^2_n}{\sqrt{n}}(c_0 \sigma^2+\sqrt{\frac{E[\xi^2]}{n^{1/2}\lambda}})$
\item For classification problem, under the same assumption as above, and taking $\lambda = \Lambda_n$, for any $\sigma \in (0,1)$, with probability at least $1-\sigma$, we have
    \begin{center}
      $\epsilon(\Hat{\eta}) \leq \epsilon(\eta ^*) + \frac{\gamma_2^2(f^*)}{\sqrt{m}}+\Hat{\gamma^{1/2}}(f^*)\frac{\ln^{1/4}{d}+\ln^{1/4}{\frac{n}{\sigma}})}{n^{1/4}}$
    \end{center}
\end{itemize}

And for numerical experiments, it has been tested from three aspects:
\begin{itemize}
\item Shape bounds for the generalization gap.

The test accuracy of the regularized and unregularized solution are generally comparable, but the values of $\frac{\|\theta\|_P}{\sqrt{n}}$ are different. Especially, the values of the unregularized models are always several orders of magnitude larger than that for the regularized models.

Also, by comparing of the path norms between the regularized and un-regularized solutions for varying widths, it shows for the un-regularized model this quantity increases with network width, whereas for the regularized model it is almost constant. 
\item Dependence on the Initialization.
By testing on $m=10000,n= 100$ and vary the variance of the random initialization $K$, it shows regularized models are much more stable than the un-regularized models. 
\end{itemize}

\subsection{A Comparative Analysis of the Optimization and Generalization Property of Two-layer Neural Network and Random Feature Models Under Gradient Descent Dynamics}

Firstly, it prove that the results of Simon Du still hold, the gradient descent dynamics still converges to a global minimum exponentially fast, regardless of the quality of the labels. And functions obtained are uniformly close to the ones found in an associated kernel method, with the kernel defined by the initialization. Secondly, it prove that for target functions in the appropriate reproducing kernel Hilbert space (RKHS), the generalization error can be made small if certain early stopping strategy is adopted for the gradient descent algorithm. The conclusion can be explicit regularization is necessary for two-layer neural network models to fully realize their potential in expressing complex functional relationships. 

The problem has been set up with the regression problem with a training data. By fitting into a two -layer neural network, the ultimate goal is to minimize the population risk and in practice, it can only work with the following empirical risk. 

To analyse the over-parameterized case, the paper has demonstrated in four aspects:
\begin{itemize}
\item properties of the initialization 
For any fixed $\sigma > 0$,with probability at least $1-\sigma$ over the random initialization, we have
    \begin{center}
      $\Hat{R_n}(\theta_0)\leq {1\2}(1+c(\sigma)\sqrt{m}\beta)^2$
    \end{center}
where $c(\sigma) = 2+\sqrt{\ln{\frac{1}{\sigma}}}$.

\item Gradient descent near the initialization 
For any fixed $\sigma \in (0,1)$, assume that $m \geq \frac{8}{\lambda_n^2}\ln(\frac{2n^2}{\sigma})$.Then with probability at least $1-\sigma$ over the random choices of $\theta_0$, we have the following holds for any $t\in[0,t_0]$,
    \begin{center}
      $\Hat{R_n}(\theta_t)\leq e^{-m(\lambda_n^{(a)}+\beta^2\lambda_n^{(b)})}t\Hat{R_n}(\theta_0)$
    \end{center}
    
\item Characterization of the whole GD trajectory
\item Curse of dimensionalality pf the implicit regularization
\end{itemize}

The numerical results to illustrate our theoretical analysis with two tests: 
\begin{itemize}
\item The first experiment studies the convergence of GD dynamics for over-parameterized two-layer neural networks with different initialization. GD algorithm for the neural network models converges exponentially fast for all initialization considered.
\item The second experiment compares the GD dynamics of two -layer neural networks and random feature models. When the width is very small, the GD algorithm for the random feature model does not converge, while it does converge for the neural network model and the resulting model does generalize. For the intermediate width, the GD algorithm for both models converges, and it converges faster for the neural network model than for the random feature model. The test accuracy is slightly better for the resulting neural network model. When width is large, the behavior of the GD algorithm for two models is almost the same. 
\end{itemize}

This paper has shown that for over-parametrized two-layer neural networks without explicit regularization, the gradient descent algorithm is sufficient for the purpose of optimization. But to obtain dimension-independent error rates for generalization, one has to require that the target function be in the RKHS with a kernel defined by the initialization. Given a target function in the Barron space, in order for implicit regularization to work, one has to know before hand that kernel function for that target function and use that kernel function to initialize the GD algorithm. This requirement is certainly impractical. In the absence of suck a knowledge, one should expect to encounter the curse of dimensionality for general target function in Barron space, as is proved in this paper. 

\section{BatchNorm}
\label{label:BatchNorm}

\subsection{Architecture of Batch Normalization}


Ioffe \& Szegedy \cite{batchnorm} originally proposed that adding batch normalization (BatchNorm) as part of the network architecture. The new BatchNorm network achieves the same accuracy on a state-of-the-art image classification model with 14 times faster.

BatchNorm is applied at each neuron individually. Figure \ref{fig:batchnorm} shows how it works on a fully-connected network. 


\begin{figure}[h]
	\centering
    \includegraphics[width=\textwidth]{pics/batchNorm/batch-normalization.jpg}
	\caption{Batch Normalization Architecture\\Source: https://www.learnopencv.com/batch-normalization-in-deep-networks/}
	\label{fig:batchnorm}
\end{figure}

\subsection{Train with Batch Normalization}
In an un-normalized network, for neuron $i$ at an arbitrary layer, $z_i=w_ix$ and $a_i=g(z_i)$ where $x\in\mathcal{R}^{d\times m}$ is a mini-batch of size $m$ $d$-dimensional input into the neuron (it could be training data or output from previous layer) and $g$ is a non-linear activation function. So $z_i$ is vector of size $m$. Bias term $b$ and layer index is omitted for simplicity. BatchNorm is added in each neuron as the brown slice shown in Figure \ref{fig:batchnorm} before non-linear activation. On each individual neuron, we calculate:

\begin{align*}
    z_i^{norm}& = \frac{z_i-\mu}{\sqrt{\sigma^2+\epsilon}}  &(1)\\
    \hat{z}_i&= \gamma_i\cdot z_i^{norm}+\beta_i  &(2)\\
    a_i&=g(\hat{z}_i) &(3)
\end{align*}

In above equations, $\gamma_i$ and $\beta_i$ are a pair of introduced parameters that to be learned. $\mu$ and $\sigma^2$ are mean and variance of the mini-batch $z_i$, respectively. They then scale and shift the normalized value $z_i^{norm}$ to ensure that the not all transformed data fall under the linear regime of non-linear activation. $\epsilon$ is added for numerical stability of the calculation (typical value is $10^{-8}$). Note that at each step of mini-batch training, the mean and variance of $z_i$ need to be recalculated.

\subsection{Gradient Descent of Batch Normalization}

During training phase, the chain rule is still applicable to a BatchNorm network. For an arbitrary loss function $l$, the chain rule is as follows:

\begin{align*}
   \frac{\partial l}{\partial z_i^{norm}} &= \frac{\partial l}{\partial \hat{z}_i}\cdot\gamma_i\\
   \frac{\partial l}{\partial \sigma^2}  &= \sum_{i=1}^m\frac{\partial l}{\partial z_i^{norm}}\cdot(z_i-\mu)\cdot\frac{-1}{2}(\sigma^2+\epsilon)^{-3/2} \\
   \frac{\partial l}{\partial \mu}  &= \sum_{i=1}^m\frac{\partial l}{\partial z_i^{norm}}\cdot \frac{-1}{\sqrt{\sigma^2+\epsilon}}\\
   \frac{\partial l}{\partial z_i}  &=  \frac{\partial l}{\partial z_i^{norm}} \cdot\bigg(\sqrt{\sigma^2+\epsilon}\bigg)^{-1} + \frac{\partial l}{\partial \sigma^2}\cdot\frac{2(z_i-\mu)}{m} +  \frac{\partial l}{\partial \mu}\cdot\frac{1}{m}\\
   \frac{\partial l}{\partial \gamma_i}  &=  \sum_{i=1}^m\frac{\partial l}{\partial \hat{z}_i}\cdot z_i^{norm} \\
   \frac{\partial l}{\partial \beta_i}  &= \sum_{i=1}^m\frac{\partial l}{\partial \hat{z}_i} \\
\end{align*}

Therefore, we can still use backpropagation to update all the parameters in a BatchNorm neural network.

\subsection{Inference with Batch Normalization}

Suppose the whole training data are split into $B$ mini-batches. When inference with BatchNorm network, deterministic mean and variance, $\mu$ and $\sigma^2$ in Eq. (1) independent of mini-batch are desired. To achieve a fixed mean and variance, once training is completed, with frozen parameters, the $B$ training mini-batches are re-processed through the trained network. Then for neuron $i$, the mean for mini-batch $k\in\{1,2,\cdots,B\}$ is denoted as $\mu_k$ and the variance as $\sigma^2_k$. Then let

\begin{align*}
   M &= \frac{1}{B}\sum_{k=1}^B\mu_k\\
   V &= \frac{m}{m-1}\Bigg(\frac{1}{B}\sum_{k=1}^B\sigma^2_k\Bigg)\\
\end{align*}

, where $M$ is the average of the means of mini-batches and $V$ is unbiased estimate of the average variance of mini-batches. Therefore, when doing inference on test data, Eq. (1) is rewritten as:

\begin{align*}
    z_i^{norm}& = \frac{z_i-M}{\sqrt{V+\epsilon}}
\end{align*}


\subsection{Loss-Landscape Smoothing} \label{landsmooth}
In Santurkar, et al, \cite{landscape} they show that landscape of loss function is smoother in BatchNorm network. BatchNorm reduces effective Lipschitzness constant and improve smoothness of the gradient.

\subsubsection{Observations}

In this paper \cite{landscape}, they perform experiments with two deep architectures to observe, a VGG-type convolutional network with 11-19 layers (exact number of layers is not specified in the paper) on CIFAR10 dataset and a 25-layer deep linear network (DLN) on synthetic Gaussian data. They also explicitly add time-varying noise of non-zero mean and non-unit variance to each batch-normalized node independently (referred as "noisy" BatchNorm model in the paper) so that distribution of activation on each node in a mini-batch is ensured to be unstable. From the performance of VGG network, shown in Figure \ref{fig:vgg}, they observe BatchNorm and noisy BatchNorm have comparable performance, they both converge faster than standard "vanilla" network. However, Figure \ref{fig:vgg} shows that although "noisy" BatchNorm achieved faster training speed than standard "vanilla" network and comparable performance as BatchNorm, the weight distribution of "noisy" BatchNorm  is more noisy than the standard one which indicate a large ICS as defined in Ioffe \& Szegedy, 2015 \cite{batchnorm}. Also, notice that in Layer \#13 the weight distribution of BatchNorm has more ICS than standard network.

\begin{figure}[h] 
	\centering
    \includegraphics[scale=0.6]{pics/batchNorm/Santurkar_fig2.jpg}
	\caption{VGG Performance}
	\label{fig:vgg}
\end{figure}

The above findings is hard to align with what is claimed in Ioffe \& Szegedy, 2015  \cite{batchnorm} that BatchNorm helps stabilize internal weight distribution so to reduce ICS. To investigate why is the case that BatchNorm improve training process, they analyze the optimization landscape in VGG networks and show it in Figure \ref{fig:optimizationlandscape}.

\begin{comment}
To further quantify the findings, a broader notation of ICS of activation i at time t is defined as $||G_{t,i}-G^{'}_{t,i}||_2$ in the paper (Santurkar, 2018), where
\begin{align*}
	G_{t,i}&=\nabla_{W_{i}^{(t)}}\mathcal{L}(W_{1}^{(t)},\cdots,W_{k}^{(t)};x^{(t)},y^{(t)})\\
	G^{'}_{t,i}&=\nabla_{W_{i}^{(t)}}\mathcal{L}(W_{1}^{(t+1)},\cdots,
	W_{i-1}^{(t+1)},W_{i}^{(t)},W_{i+1}^{(t)},\cdots,W_{k}^{(t)}
	;x^{(t)},y^{(t)})
\end{align*}

In above definition, $W_{1}^{(t)},\cdots,W_{k}^{(t)}$ are weights of each of the $k$ layers at time $t$. $G_{t,i}$ is the gradient of the loss before applying any weights update and $G^{'}_{t,i}$ is the gradient after updating the first $i-1$ layers. So, $l_2$ difference of $G$ and $G'$ corresponds to the change in the optimization landscape of $W_i$ caused by its input change.

\begin{figure} 
	\centering
    \includegraphics[width=\textwidth]{pics/batchNorm/Santurkar_fig7.jpg}
	\caption{ICS}
	\label{fig:ICSshift}
\end{figure}

Regardless of the difference in definition, they measure cosine angle (ideal value is 1) and $l_2$-difference (idea value is 0) of the gradients and plot it in Figure \ref{fig:ICSshift}. They claim that the figure shows that BatchNorm does not clearly improve ICS. DLN has even worse ICS with BatchNorm.
\end{comment}

\subsubsection{Theory Results}
At each training step, suppose the weights is at position A of the optimization landscape, they calculate the gradient at position A, then they "walk" in the gradient direction of A and mark some positions along the way. In graph (a) of Figure \ref{fig:optimizationlandscape}, shaded area is the change in loss between A and each of the marked positions. In graph (b) the shaded area is $l_2$-difference of the gradient at A and the gradient at each of the marked positions. Graph (c) shows the maximum value of gradient difference (as $l_2$-norm) divided by distance from A to corresponding marked position. The maximum value can be seen as "effective" Lipschitz constant. It is evident that BatchNorm improves smoothness of the optimization landscape.

\begin{figure}[h]
	\centering
    \includegraphics[width=\textwidth]{pics/batchNorm/Santurkar_fig4.jpg}
	\caption{Optimization Landscape}
	\label{fig:optimizationlandscape}
\end{figure}

In following section, they analyze the phenomenon theoretically. In the theoretical setup, a single BatchNorm layer is arbitrarily inserted at a fully-connected layer. Layer weights are denoted as $W_{ij}$. An arbitrary unnormalized loss function at current layer as $\mathcal{L}$ and normalized loss $\hat{\mathcal{L}}$ that could have downstream non-linear layers. For input $x$ let $y=Wx$. $\hat{y}=\text{BN}(y)$ is batch-normalized $y$ that has mean 0 and variance 1. Then $z=\gamma\hat{y}+\beta$ where $\gamma$ and $\beta$ are assumed to be constant for the following analysis. 

\textbf{Theorem 4.1} shows that BatchNorm improve Lipschitzness of the loss with respect to $y_j$.

\begin{align*}
	||\nabla_{y_j}\hat{\mathcal{L}}||^2&\leq\frac{\gamma^2}{\sigma^2_j}\bigg(||\nabla_{y_j}\mathcal{L}||^2
	- \frac{1}{m} \langle \mathbf{1},\nabla_{y_j}\mathcal{L} \rangle^2 
	- \frac{1}{m}\langle \nabla_{y_j}\mathcal{L}, \hat{y}_j \rangle^2 \bigg)\\
\end{align*}

, where $\sigma_j^2$ is variance of a batch of outputs $y_j\in\mathrm{R}^m$ and $j$ denotes an arbitrary node in the normalized layer. 

The value of $\sigma_j^2$ tends to be large in practice. The inequality utilizes an assumption that $\partial \hat{\mathcal{L}}/\partial z_j = \partial \mathcal{L}/\partial y_j$. The second term, $ \langle \mathbf{1},\nabla_{y_j}\mathcal{L} \rangle^2=\big( \sum_{k=1}^{m}\partial \mathcal{L}/\partial y_j^{(k)} \big)$, is non-negative and most likely greater than zero. The third term indicates correlation of variable $\hat{y}_j$ and $\nabla_{y_j}\mathcal{L}$. Since they both correlates to $y_j$, this term is also greater than zero. Therefore,  Lipschitzness of the loss with respect to $y_j$ is proved to be improved by BatchNorm.

\begin{comment}
Next they focus on the $2$nd-order properties of the loss landscape w.r.t $y_j$. Let $\mathbb{\textit{H}}_{jj}=\frac{\partial\mathcal{L} }{\partial y_j\partial y_j}$ and $\hat{g}_j = \nabla_{y_j}\mathcal{L}$ be the Hessian and gradient of the loss w.r.t. the layer outputs. They show that:

\begin{align*}
	\bigg(\nabla_{y_j}\hat{\mathcal{L}}\bigg)^T\frac{\partial \hat{\mathcal{L}}}{\partial y_j\partial y_j} \bigg(\nabla_{y_j}\hat{\mathcal{L}}\bigg) &\leq 
	\frac{\gamma^2}{\sigma^2}\Bigg[
	\bigg(\frac{\partial \hat{\mathcal{L}}}{\partial y_j}\bigg)^T\mathbb{\textit{H}}_{jj}\bigg(\frac{\partial \hat{\mathcal{L}}}{\partial y_j}\bigg) -
	\frac{1}{m\gamma}\langle\hat{g}_j,\hat{y}_j\rangle\bigg|\bigg|\frac{\partial \hat{\mathcal{L}}}{\partial y_j}\bigg|\bigg|^2
	\Bigg]\\
\end{align*}

If $\mathbb{\textit{H}}_{jj}$ preserves the relative norms of $\hat{g}_j$ and $\nabla_{y_j}\hat{\mathcal{L}}$,


\begin{align*}
	\bigg(\nabla_{y_j}\hat{\mathcal{L}}\bigg)^T\frac{\partial \hat{\mathcal{L}}}{\partial y_j\partial y_j} \bigg(\nabla_{y_j}\hat{\mathcal{L}}\bigg) &\leq 
	\frac{\gamma^2}{\sigma^2}\Bigg(
	\hat{g}_j^T \mathbb{\textit{H}}_{jj} \hat{g}_j -
	\frac{1}{m\gamma}\langle\hat{g}_j,\hat{y}_j\rangle\bigg|\bigg|\frac{\partial \hat{\mathcal{L}}}{\partial y_j}\bigg|\bigg|^2
	\Bigg)\\
\end{align*}

The above inequality holds true if it satisfies two conditions: 1. the loss is locally convex (it is true for deep network with piecewise-linear activation functions) so the Hessian is positive semi-definite. 2. Negative gradient $\hat{g}_j$ points to the minimum of the loss so the inner product $\langle\hat{g}_j,\hat{y}_j\rangle>0$. They claim that both conditions are fairly mild assumptions. The quadratic Hessian term is the second order term of the Taylor expansion of loss around the current position. When this term is reduced by BatchNorm, it is more accurate to predict the loss around the current position by using only the first order term.
\end{comment}
Next, in \textbf{Theorem 4.4}, they transfer the above theorem to one that relates to layer weights by using chain rule $\frac{\partial\hat{L}}{\partial W_{.j}}=\textbf{X}^T\big(\frac{\partial\hat{L}}{\partial y_{j}}\big)$. Let $g_j=\underset{||X||\leq\lambda}{\mathrm{max}}\big|\big|\nabla_W\mathcal{L}\big|\big|^2$, $\hat{g}_j=\underset{||X||\leq\lambda}{\mathrm{max}}\big|\big|\nabla_W\hat{\mathcal{L}}\big|\big|^2$ and $\mu_{g_j}=\frac{1}{m}\langle\mathbf{1},\partial\mathcal{L}/\partial y_j\rangle$ then

\begin{align*}
	\hat{g}_j &\leq 
	\frac{\gamma^2}{\sigma_j^2}\Bigg(
	g_j^2 - m\lambda^2\mu^2_{g_j}-\frac{\lambda^2}{m}\langle\nabla_{y_j}\mathcal{L},\hat{y}_j\rangle^2
	\Bigg)\\
\end{align*}

(Note that in original paper, $\lambda^2$ is missing in the second term and $1/m$ is missing in the third term.)

The above theorem shows that worst-case bound of the loss gradient w.r.t. layer weights also improves by applying BatchNorm.

\subsubsection{Discussion}

This paper both empirically and theoretically shows that optimization landscape is smoothed by BatchNorm layer. BatchNorm improves Lipschitzness of gradient of arbitrary loss of deep neural network. However, in this paper the proof is based on an assumption that $\partial\hat{\mathcal{L}}/\partial z_j = \partial\mathcal{L}/\partial y_j$. Further proof is needed for this assumption. Moreover, in the experiment section one of the networks is deep linear network (DLN). Although there are researchers studying DLN for simplicity and claiming that DLN resembles some properties of DNN \cite{dln}, formal theories are needed to explain how well DLN can be generalized to deep neural networks.

% Deep linear network is not discussed in original BatchNorm paper (Ioffe \& Szegedy, 2015), and properties of the simplified architecture of DLN is yet to fully studied. Therefore, this report will focus on the more practical architecture, VGG network.

\subsection{BatchNorm-Length-direction decoupling}

In paper \cite{decoupling} they show that BatchNorm is equivalent to rewrite weights in a way that decoupling its length and direction. Therefore, the two parameters of weights can be trained separately.

Suppose for a binary classification problem, training data input $\textbf{x}\in\mathbb{R}^d$, label $y\in{\pm1}$. Let $\textbf{z}=-y\textbf{x}$, $\textbf{u}:=\mathbb{E}[\textbf{z}]$, $\textbf{S}:=\mathbb{E}[\textbf{xx}^T]$, $\textbf{w}$ is weight vector on an arbitrary unit on input x and $\varphi: \mathbb{R}\xrightarrow{}\mathbb{R}$ is non-linear function. Then output $f(\textbf{w})$ of an unit on an input \textbf{x} is:

\begin{align*}
	f(\textbf{w})=\mathbb{E}_{\textbf{x}}\big[\varphi\big(\textbf{x}^T\textbf{w}\big)\big]\\
\end{align*}

When applied BatchNorm to the unit before activation, the output becomes:

\begin{align*}
	f_{BN}(\textbf{w},\gamma,g)&=\mathbb{E}_{\textbf{x}}\big[\varphi\big(BN\big(\textbf{x}^T\textbf{w}\big)\big)\big]\\
	BN\big(\textbf{x}^T\textbf{w}\big)&=g\frac{\textbf{x}^T\textbf{w}-\mathbb{E}_{\textbf{x}}[\textbf{x}^T\textbf{w}]}{var_\textbf{x}[\textbf{x}^T\textbf{w}]^{1/2}}+\gamma\\
\end{align*}
Assume $\textbf{x}$ is zero mean and omit $\gamma$:
\begin{align*}
	f_{BN}(\textbf{w},g)&=\mathbb{E}_{\textbf{x}}\Big[\varphi\Big(g\frac{\textbf{x}^T\textbf{w}}{(\textbf{w}^T\textbf{Sw})^{1/2}}\Big)\Big]
\end{align*}

Let $||\textbf{w}||_{\textbf{S}}=(\textbf{w}^T\textbf{Sw})^{1/2}$ and define $\Tilde{\textbf{w}}:=g\frac{\textbf{w}}{||\textbf{w}||_{\textbf{S}}}$. Then we can write  $f_{BN}(\textbf{w},g)$ as:

\begin{align*}
    f_{BN}(\textbf{w},g)&=\mathbb{E}_{\textbf{x}}\Big[\varphi\big(\textbf{x}^T\Tilde{\textbf{w}}\big)\Big]
\end{align*}

Therefore, BatchNorm can be seen as reparameterizing the weight space.

\subsubsection{Theory Resuts}

In this paper they make four assumptions for later theoretical analysis:

\textbf{Assumption 1}: $\mathbb{E}_{\textbf{x}}[\textbf{x}]=0$; minimum and maximum eigenvalues of $\textbf{S}$ are $\mu>0$ and $L<\infty$, respectively. Therefore, $\textbf{S}$ is positive-definite covariance matrix of $\textbf{x}$.

\textbf{Assumption 2}: $\textbf{z}=-y\textbf{x}$ is a multivariate Gaussian random variable with mean $\textbf{u}$ and variance $\textbf{S}-\textbf{uu}^T$.

\textbf{Assumption 3}: loss function $\varphi:\mathbb{R}\rightarrow{}\mathbb{R}$ is infinitely differentiable with a bounded derivative.

\textbf{Assumption 4}: output function $f:\mathbb{R}^d\rightarrow{}\mathbb{R}$ is $\zeta$-smooth if it is differentiable on $\mathbb{R}$ and its gradient is $\zeta$-Lipschitz. Also, they assume that $\alpha_*:=\mathrm{argmin}_{\alpha}||\nabla f(\alpha\textbf{w})||^2$ exists and is finite for all $\textbf{w}$.

Coming to the theoretical part, the architecture setup they use is a one hidden layer of $m$ neurons fully-connected neural network. For a loss function $l:\mathbb{R}\rightarrow{}\mathbb{R}^+$ and $F(\textbf{x},\Tilde{\textbf{W}}):=\sum_{i=1}^m\theta_i\varphi(\textbf{x}^T\Tilde{\textbf{w}}^{(i)})$ with all $\theta_i$ frozen. The goal is:\\

\begin{align*}
\underset{\Tilde{\textbf{W}}}{\min}\Bigg(f_{NN}(\Tilde{\textbf{W}}):=\mathbb{E}_{y,\textbf{x}}\Big[l\Big(-yF(\textbf{x},\Tilde{\textbf{W}})\Big)\Big]\Bigg)
\end{align*}

In the paper an odd function, tanh, is used as activation function. Therefore, the above equation can be rewritten as:

\begin{align*}
\underset{\Tilde{\textbf{W}}}{\min}\Bigg(f_{NN}(\Tilde{\textbf{W}})=\mathbb{E}_{\textbf{z}}\Big[l\Big(F(\textbf{z},\Tilde{\textbf{W}})\Big)\Big]\Bigg)
\end{align*}

They first show in \textbf{Lemma 2} that if Assumptions 1 \& 2 are true, then a critical point (local/global minimum) $\hat{\textbf{w}}^{(i)}$ for a hidden unit $i$ satisfies that $\forall m=1,\cdots,m$ and a scalar $\hat{c}^{(i)}$.
\begin{align*}
\hat{\textbf{w}}^{(i)}=\hat{c}^{(i)}\textbf{S}^{-1}\textbf{u}
\end{align*}
The above lemma implies that given Gaussian inputs, critical points of all hidden neurons are along the same direction in $\mathcal{R}^d$ space. The scalar $\hat{c}^{(i)}$ depends on $\alpha, \beta$ and $\gamma$ given in later part. Furthermore, the direction of a critical point is only dependant on input into the layer. 

\begin{figure}[h]
\begin{tabular}{cc}
  \includegraphics[scale=0.38]{pics/batchNorm/decoupling_alg1.jpg} & 
  \includegraphics[scale=0.58]{pics/batchNorm/decoupling_alg2.jpg}
  \\
(1) GDNP Algorithm & (2) Training MLP with GNDP\\[6pt]
\multicolumn{2}{c}{\includegraphics[scale=0.6]{pics/batchNorm/decoupling_alg3.jpg} }\\
\multicolumn{2}{c}{(3) Bisection Algorithm }
\end{tabular}
\caption{Training Algorithm}
\label{fig:algorithms}
\end{figure}

Next, they propose a training algorithm for analysis purpose, shown in Figure \ref{fig:algorithms}. The algorithm optimizes each neuron independently and sequentially (in an order of given neuron indexes). The algorithm also leverages the above \textbf{Lemma 2} decoupling input and output of an unit. Then they prove that, if all four assumptions hold, by training with the proposed algorithm (GDNP), an exponential convergence rate can be achieved:

\begin{align*}
    ||\nabla_{\Tilde{\textbf{w}}^{(i)}}f\big(\Tilde{\textbf{w}}_t^{(i)}\big)||^2_{\textbf{S}^{-1}}&\leq(1-\mu/L)^{2t}C(\rho(\textbf{w}_0)-\rho(\textbf{w}^*))+2^{-T_s^{(i)}}\zeta|b_t^{(0)}-a_t^{(0)}|/\mu^2\\\\
    \mathrm{, where\ }\Tilde{\textbf{w}}:&=g\frac{\textbf{w}}{||\textbf{w}||_{\textbf{S}}} \mathrm{\ and\ step\ size\ } s_t^{(i)}=||\textbf{w}_i^{(i)}||^3_{\textbf{S}}/(L\theta^{(i)}g_t^{(i)}\xi_t\textbf{u}^T\textbf{w}_t^{(i)})\\
    \zeta\mathrm{\ is\ }&\mathrm{Lipschitz\ constant\ of\ }f\\\\
    C&=2\Phi^2+2i\sum_{j<i}(\theta^{(j)}c_j)^2>0\\
    \rho(\textbf{w}):&=-\frac{\textbf{w}^T\textbf{uu}^T\textbf{w}}{\textbf{w}^T\textbf{Sw}}\\\\
    \xi_t&=\alpha_t^{(i)}+\sum_{j,i}\gamma_t^{(i,j)}c_j\\
    \alpha^{(i)}:&=\mathbb{E}_{\textbf{z}}\Big[l^{(1)}(F(\textbf{z},\Tilde{\textbf{W}}))\varphi^{(1)}(\textbf{z}^T\Tilde{\textbf{w}}^{(i)})\Big]-\sum_{j=1}^m\gamma^{(i,j)}(\textbf{u}^T\Tilde{\textbf{w}}^{(j)})\\
    \beta^{(i)}:&=\mathbb{E}_{\textbf{z}}\Big[l^{(1)}(F(\textbf{z},\Tilde{\textbf{W}}))\varphi^{(2)}(\textbf{z}^T\Tilde{\textbf{w}}^{(i)})\Big]\\
    \gamma^{(i,j)}:&=\theta^{(j)}\mathbb{E}_{\textbf{z}}\big[l^{(2)}(F(\textbf{z},\Tilde{\textbf{W}}))\varphi^{(1)}(\textbf{z}^T\Tilde{\textbf{w}}^{(i)})\varphi^{(1)}(\textbf{z}^T\Tilde{\textbf{w}}^{(j)})\big]\\
    \Tilde{\textbf{W}}:&=\{\Tilde{\textbf{w}}^{(1)},\cdots,\Tilde{\textbf{w}}^{(m)}\}
\end{align*}

When updating current weight parameters $(\textbf{w}^{(i)}, g^{(i)})$, they further assume that all updated neurons weights, $\{\textbf{w}^{(k)}\}_{k<i}$, are at critical points and neurons to be updated all have weight $\textbf{w}^{(k)}=\textbf{0}$ for $k>i$.

\begin{comment}
Based on the fact that

\begin{align*}
    ||\nabla_{\Tilde{\textbf{w}}^{(i)}}f\big(\Tilde{\textbf{w}}_t^{(i)}\big)||^2_{\textbf{S}^{-1}} &= ||\textbf{w}_t^{(i)}||^2_{\textbf{S}}||\nabla_{\textbf{w}^{(i)}}f(\textbf{w}_t^{(i)},g^{(i)})||^2_{\textbf{S}^{-1}}/(g_t^{(i)})^2\\
    &+\Big(\partial_{g^{(i)}}f(\textbf{w}_t^{(i)},g^{(i)}_{t})\Big)^2
\end{align*}
\end{comment}
The above convergence result is actually a composition of two parts in below: convergence of scalar $g$ and the direction $\textbf{w}$. Note that both of the two parts have exponential convergence rate.

\textbf{Convergence of scalar} $g^{(i)}$:

\begin{align*}
    \Big(\partial_{g^{(i)}}f(\textbf{w}_t^{(i)},g^{(i)}_{T_s})\Big)^2\leq2^{-T_s^{(i)}}\zeta|b_t^{(0)}-a_t^{(0)}|/\mu^2
\end{align*}

\textbf{Convergence of the direction} $\textbf{w}^{(i)}$:

\begin{align*}
    ||\textbf{w}_t^{(i)}||^2_{\textbf{S}}||\nabla_{\textbf{w}^{(i)}}f(\textbf{w}_t^{(i)},g_t^{(i)})||^2_{\textbf{S}^{-1}}\leq (1-\mu/L)^{2t}\xi_t^{2}g_t^{2}(\rho(\textbf{w}_0)-\rho(\textbf{w}^*))\\
\end{align*}

\subsubsection{Experiment}

In the experiment section, they train a neuron network of 6 hidden layers with 50 neurons each on CIFAR-10 image classification data. Note that the input data is no longer multivariate-Gaussian variables. All hidden layers are batch-normalized. For comparison, they also train an un-normalized network of the same structure. Both networks are trained by standard gradient descent and use tanh as middle layers activation. To validate \textbf{Lemma 2} that direction of critical points is independent of downstream layers, they calculate Frobenius norm of $\frac{\partial^2f_{NN}}{\partial\textbf{W}_4\partial\textbf{W}_i}$ as a measurement of cross-dependency of layer-4 with other layer, layer-$i$. Their experimental results are shown in Figure \ref{fig:decouplingexperiment}. The top two graphs clearly shows that BatchNorm network do achieve faster convergence rate. Also, the bottom two graphs shows that in BatchNorm network, dependence of a middle layer on its downstream layers (layer-5 and layer-6) becomes weaker as the layer is further away from it.

\begin{figure} 
	\centering
    \includegraphics[width=\textwidth]{pics/batchNorm/decoupling_experiment.jpg}
	\caption{Experimental Results: Top two plots show the results of decay of loss function and norm of the loss gradient. Bottom two plots show cross-dependency in BatchNorm network and un-normalized network, respectively.}
	\label{fig:decouplingexperiment}
\end{figure}

\subsubsection{Discussion}

As for the four assumptions, the first one is feasible when input data subtracted from its mean. Assumption 2 evidently does not always hold. However, in the experiment that using non-Gaussian data, they show that the results are still valid. Assumption 3 is valid for tanh and sigmoid activations. ReLu is not differentiable at 0. However, the probability that a point is exactly zero is provable zero. Therefore, Assumption 3 is also feasible for the three enumerated popular activation functions. Last, validity of Assumption 4 is due to further proof. 

\subsection{Variants to Batch Normalization}

\subsubsection{Limitations of Batch Normalization}

Although BatchNorm is a powerful technique that speed up training process, it still has its own limitations. The mean and variance of each neuron in BatchNorm depends on size of min-batch and vary from mini-batch to min-batch. That would introduce certain amount of errors and propagate through the network which is harmful to noise sensitive applications like reinforcement learning \cite{reparameter}. Also, variance of the estimated population mean and variance is increased when mini-batch size is small. Therefore, when applying BatchNorm, the choice of mini-batch size should be considered carefully. Moreover, BatchNorm is not suitable for recurrent model like RNNs which limits application of such powerful technique.

\subsubsection{Weight Normalization}
Paper \cite{reparameter} proposed Weight Normalization (WeightNorm), another way of neural network parameterization. In their paper, weight vector is expressed as:

\begin{align*}
    \textbf{w}=\frac{g}{||\textbf{v}||}\textbf{v}
\end{align*}

where $g$ is a scalar, $\textbf{v}$ is a vector and $||\textbf{v}||$ is $l_2$ norm of $\textbf{v}$. Gradient of loss function $l$  for a WeightNorm network with respect to the new parameters $g$ and $\textbf{v}$ is:

\begin{align*}
    \nabla_gl &= \frac{\nabla_{\textbf{w}}l\cdot\textbf{v}}{||\textbf{v}||}\\
    \nabla_{\textbf{v}}l &= \frac{g}{||\textbf{v}||}\nabla_{\textbf{w}}l-\frac{g\nabla_gl}{||\textbf{v}||^2}\textbf{v}
\end{align*}

They also propose a "Mean-only" Batch Normalization adopted into the WeightNorm method by which pre-activation $t=\textbf{W}\cdot\textbf{x}$ is subtracted from its mean (without scaled by its standard deviation) then applied to non-linear activation $y=\phi(t-\mu[t])$. Parameter $\textbf{w}$ is written as WeightNorm expression. The "Mean-only" BatchNorm has an effect of centering gradient which is a low-cost operation. It their experiments, "Mean-only" WeightNorm achieves lowest test error among WeightNorm, BatchNorm and regular parameterization on a CNN network. They also apply regular WeightNorm to deep convolutional variantional auto-encoders (CVAEs), a recurrent variational autoencoder with generative model DRAW and  a Deep Q-Network (DQN) for reinforcement learning. In all three models, WeightNorm outperform normal parameterization in terms of both convergence rate and evaluation metric.

\subsubsection{Layer Normalization}
Paper \cite{layernorm} propose Layer Normalization (LayerNorm) that can be applied to recurrent-type network. Figure \ref{fig:batchvslayernorm} shows a comparison of BatchNorm and LayerNorm.  In an arbitrary layer, LayerNorm is applied on each input independently. 

\begin{figure}[h]
	\centering
    \includegraphics[scale=0.5]{pics/batchNorm/BatchNorm_vs_LayerNorm.jpg}
	\caption{Batch Normalization vs. Layer Normalization}
	\label{fig:batchvslayernorm}
\end{figure}

In a standard RNN, at time-step $t$ suppose current input $\textbf{x}^t$ and previous hidden state output $\textbf{h}^{t-1}$, let $\textbf{a}^t \in \mathbb{R}^K = W_{hh}\textbf{h}^{t-1}+W_{xh}\textbf{x}^t$, then output $\textbf{h}^t$ can be expressed as:

\begin{align*}
    \textbf{h}^t&=\sigma\Big(\frac{\textbf{g}}{\sigma^t}\odot(\textbf{a}^t - \mu^t) + \textbf{b}\Big)\\
    \mu^t&=\frac{1}{K}\sum_{i=1}^Ka_i^t\\
    \sigma^t&=\sqrt{\frac{1}{K}\sum_{i=1}^K(a_i^t-\mu^t)^2}
\end{align*}

They test with layer normalization on 5 tasks implementing RNNs. All 5 tasks converge faster than baseline model with at least comparable results (LayerNorm perform better on 3 of the 5 tasks).

\subsubsection{Group Normalization}

Small mini-batch size rapidly increases BatchNorm’s error due to inaccurate estimation of statistics. Thus, applying BatchNorm in large memory-consuming models such as computer vision tasks is limited especially for image detection, segmentation, and video classification. 

Paper \cite{groupnorm} propose Group Normalization (GroupNorm) which is independent of mini-batch size to address this issue. Implementation of GroupNorm is shown in Figure \ref{fig:groupnorm}. Calculation of the mean and variance is on each individual input across entire height and width of subset of channels in a layer. The formulation is in the same form as BatchNorm with the difference is the terms that being summed over. 

\begin{figure}[h]
	\centering
    \includegraphics[scale=0.7]{pics/batchNorm/GroupNorm.jpg}
	\caption{Group Normalization: N - Mini-batch size; C - number of channels, H,W - height \& width of a feature map being flatten. Normalization is taken over the blue region.}
	\label{fig:groupnorm}
\end{figure}

Figure \ref{fig:resnet} shows the dependence of BatchNorm's performance on mini-batch size while GroupNorm doesn't have such dependence. The performance of GroupNorm is comparable as BatchNorm with larger mini-batch sizes. As for number of channels per group, they show that in ResNet-50 performance are well for all values that are studied from 2 to 64. The value can also vary from layer to layer which is another flexibility of GroupNorm.

\begin{figure}[h]
	\centering
    \includegraphics[scale=0.57]{pics/batchNorm/GroupNorm_vs_BatchNorm.jpg}
	\caption{ResNet-50 trained on ImageNet set}
	\label{fig:resnet}
\end{figure}

\subsubsection{Summary}
Table~\ref{normsum} shows a summary of all mentioned normalization methods along with their applications and way of calculation.\\

\begingroup
\setlength{\tabcolsep}{10pt} % Default value: 6pt
\renewcommand{\arraystretch}{1.5} % Default value: 1
\begin{table}[h] 
\centering
 \begin{tabular}{|m{10em} | m{10em} | m{10em}|} 
 \hline
 Method & Application & Calculation\\  
 \hline
 Batch Normalization & Fully Connected Layers & Individual Neuron, over mini-batch input\\ [0.5ex] 
 \hline
 Layer Normalization & Recurrent Layers & Individual Input, over all neurons in a layer\\ [0.5ex]
 \hline
 Group Normalization & Convolution Layers & Individual Input, over entire height \& weight in a group of channels\\[0.5ex]
 \hline
 Weight Normalization & Any Architecture & Decouple length and direction of weights\\
 \hline
\end{tabular}
\caption{Summary of Normalization}
\label{normsum}
\end{table}
\endgroup

\subsection{Our Own Experiments}
The first experiment conducted is a 4-layer fully connected feed-forward network with 32-64-32 neurons in each hidden layer. Output layer is a softmax layer of 10 neurons. Dataset is fashion-mnist, $28\times28$ grey scale pictures of 10 categories. BatchNorm is added to all three hidden layers as our BatchNorm network.

To verify weather Lipschitzness of the loss landscape is improved by BatchNorm as stated in Section \ref{landsmooth}, we plot the maximum loss gradient norm among batches with respect to the second last layer (calculating loss gradient norm over the entire weight space would require enormous amount of computation time, so for simplicity the calculation is only taken over one layer), shown in Figure \ref{fig:maxgradient}. The simple network architecture converges fast and later epochs indicates overfitting. So only the early epochs is presented in the graph. The top two graphs using Relu activation show that BatchNorm network has smaller gradient norm than "vanilla" network. However, the bottom two show that BatchNorm network using Tanh activation has higher gradient norm. It worth noting that results are consistent regardless of combination of learning rate and weight initialization of our choice.

\begin{figure}[h]
\begin{tabular}{cc}
  \includegraphics[scale=0.48]{pics/batchNorm/BatchNorm_relu_1.jpg} &
  \includegraphics[scale=0.52]{pics/batchNorm/BatchNorm_relu_5.jpg}
  \\
  (1) Learning Rate=1e-3, Relu & (2) Learning Rate=5e-3, Relu\\[6pt]
  \includegraphics[scale=0.5]{pics/batchNorm/BatchNorm_tanh_1.jpg} &
  \includegraphics[scale=0.5]{pics/batchNorm/BatchNorm_tanh_5.jpg}\\
  (3) Learning Rate=1e-3, Tanh & (4) Learning Rate=5e-3, Tanh\\[6pt]
\end{tabular}
\caption{Maximum gradient norm among batches w.r.t. $W$ in last dense layer. Legend in each graph shows if BatchNorm is applied in the network, learning rate, weight initialization method and activation function. The same random seed for weight initialization is used across all runs to ensure a better control of the experiments.}
\label{fig:maxgradient}
\end{figure}

Looking again at the theory setup in Section \ref{landsmooth}, notice that output of a neuron is $y=Wx$ which omits non-linear activation function. However, if Relu is used as activation function, it can be seen as a linear calculation on positive $Wx$. For negative $Wx$, the gradient on the corresponding $W$ is zero, so those weights will not contribute to norm calculation. Therefore, gradient norm in a Relu layer is expected to satisfy \textbf{Theorem 4.4} . This conjecture can be verified on the top two graphs of Figure \ref{fig:maxgradient}. 

On the other hand, Tanh activation function can not be seen as a linear calculation. Its derivative is more complicated, therefore we cannot compare the gradient norm of a Tanh network with a linear network. It's an explanation that the behaviour of the bottom two graphs does not align with the theory.

Another observation is that the training time of BatchNorm network is twice as much as the regular network. When training a high-capacity network, since it's been observed in other papers' experiments that BatchNorm reduces the training epochs by a factor much larger than 2, the overall training speed is expected to be effectively improved by BatchNorm.

\newpage
\section*{References}

References follow the acknowledgments. Use unnumbered first-level heading for
the references. Any choice of citation style is acceptable as long as you are
consistent. It is permissible to reduce the font size to \verb+small+ (9 point)
when listing the references. {\bf Remember that you can use more than eight
  pages as long as the additional pages contain \emph{only} cited references.}
\medskip

\small

\begin{thebibliography}{9}
\bibitem{batchnorm} 
S. Ioffe \& C. Szegedy (2015) Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift \textit{ICML}
 
\bibitem{landscape} 
S. Santurkar, D. Tsipras, A.Ilyas \& A. Madry (2018) How Does Batch Normalization Help Optimization? \textit{32nd Conference on In NeurIPS}

\bibitem{dln} 
A. M. Saxe, J. Mcclelland \& S. Ganguli (2014). Exact solutions to the nonlinear dynamics of learning in deep linear neural networks.
 
\bibitem{decoupling} 
J. Kohler, H. Daneshmand, A. Lucchi et. al. (2019) Exponential convergence rates for Batch Normalization:The power of length-direction decoupling in non-convex optimization. 	\textit{Proceedings of Machine Learning Research, PMLR 89:806-815.}

\bibitem{reparameter}
T. Salimans \& D.P. Kingma. (2016) Weight normalization: A simple reparameterization to accelerate
training of deep neural networks. \textit{In Advances in Neural Information Processing Systems (NIPS)}.

\bibitem{layernorm}
J.L. Ba, J.R Kiros \& G.E Hinton (2016) Layer Normalization. \textit{arXiv:1607.06450}

\bibitem{groupnorm}
Y. Wu \& K. He (2018) Group Normalization \textit{European Conference on Computer Vision (ECCV)}

\bibitem{Milman}
Milman, V. D., \& Schechtman, G. (2009). Asymptotic theory of finite dimensional normed spaces: Isoperimetric inequalities in Riemannian manifolds (Vol. 1200). Springer.

\bibitem{chen}
Chen, L., Wang, H., Zhao, J., Papailiopoulos, D., \& Koutris, P. (2018). The effect of network width on the performance of large-batch training. In Advances in Neural Information Processing Systems (pp. 9302-9309).

\bibitem{dong}
Yin, D., Pananjady, A., Lam, M., Papailiopoulos, D., Ramchandran, K., \& Bartlett, P. (2017). Gradient diversity: a key ingredient for scalable distributed learning. arXiv preprint arXiv:1706.05699.

\end{thebibliography}






\end{document}

